{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "6cab5789-88ef-4b28-ab78-8baff1aed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import queue\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "7ff0f915-36cd-454f-98fe-eca904655d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " class Value:\n",
    "        def __init__(self, value, parents=[], op=None, pow=None):\n",
    "            self.grad = 0\n",
    "            self.value = value\n",
    "            self.parents = parents\n",
    "            self.op = op\n",
    "            self.pow = pow\n",
    "        \n",
    "        def __repr__(self):\n",
    "             return f\"Value(data={self.value}, grad={self.grad})\"\n",
    "        \n",
    "        def __add__(self,other):\n",
    "            other = other if isinstance(other, Value) else Value(other)\n",
    "            return Value(value=self.value+other.value, parents=[self, other], op='+')\n",
    "                         \n",
    "        def __mul__(self,other):\n",
    "            other = other if isinstance(other, Value) else Value(other)\n",
    "            return Value(value=self.value*other.value, parents=[self, other], op='*')\n",
    "\n",
    "        def __pow__(self, other):\n",
    "            return Value(self.value**other, [self], '**', pow=other)\n",
    "        \n",
    "        def __truediv__(self, other):\n",
    "            return self * other**-1\n",
    "        \n",
    "        def __sub__(self, other):\n",
    "            return self + (-other)\n",
    "\n",
    "        def __rsub__(self, other): # other - self\n",
    "            return other + (-self)\n",
    "        \n",
    "        def __rmul__(self, other): # other * self\n",
    "            return self * other\n",
    "\n",
    "        def __radd__(self, other): # other + self\n",
    "            return self + other   \n",
    "        \n",
    "        def __neg__(self):\n",
    "            return self * -1\n",
    "        \n",
    "        def tanh(self):\n",
    "            return Value(value=math.tanh(self.value), parents=[self], op='tanh')\n",
    "        \n",
    "        def relu(self):\n",
    "             return Value(0 if self.value < 0 else self.value, parents=[self], op='ReLU')\n",
    "        \n",
    "        def compute_parents_grad(self):\n",
    "            match self.op:\n",
    "                case '**':\n",
    "                    self.parents[0].grad += self.pow * self.grad * self.parents[0].value **(self.pow - 1)\n",
    "                case '*':\n",
    "                    self.parents[0].grad += self.grad * self.parents[1].value\n",
    "                    self.parents[1].grad += self.grad * self.parents[0].value\n",
    "                case '+':\n",
    "                    self.parents[0].grad += self.grad\n",
    "                    self.parents[1].grad += self.grad                 \n",
    "                case 'ReLu':\n",
    "                    self.parents[0].grad += self.grad * (parents[0].value > 0)\n",
    "                case 'tanh':\n",
    "                    self.parents[0].grad +=  self.grad * (1 - math.tanh(self.parents[0].value)**2)\n",
    "                    \n",
    "                    \n",
    "        def backpropagate(self):\n",
    "            q = queue.Queue()\n",
    "            \n",
    "            def bfs(v):\n",
    "                q.put(v)\n",
    "                while not q.empty():\n",
    "                    top = q.get()\n",
    "                    top.compute_parents_grad()\n",
    "                    for p in top.parents:\n",
    "                        q.put(p)\n",
    "                        \n",
    "            self.grad = 1\n",
    "            bfs(self)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "d63047a8-bc00-4e15-9a3c-1f0b52110655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "     \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "            \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "c7d60abd-b09f-46d7-aa69-224f589b035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Module):\n",
    "    \n",
    "    def __init__(self, nr_in, nonlin =True):\n",
    "        self.weights = [Value(random.uniform(-1,1)) for _ in range(nr_in)]\n",
    "        self.bias = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "                         \n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "a50d30d5-93bf-4b89-ba23-d928a285dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nr_in, nr_out, **kwargs):\n",
    "        self.neurons = [Neuron(nr_in, **kwargs) for _ in range(nr_out)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "44dba203-dcbb-4b82-b2d8-07ab0dd6cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "c1d4deab-3d10-4c51-bb30-3f7aaee2439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "975a069d-84f7-4797-9b8a-11c4996caf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.7882574558440452, grad=0)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "b9204be1-a75b-4051-9f1d-2f4f6194253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4,4,1])\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "099ffa75-349a-41e7-b1e0-5be8d9325d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.716898246828779e+20\n",
      "1 1.5539516537844654e+21\n",
      "2 2.7702121487743304e+21\n",
      "3 4.938426063981843e+21\n",
      "4 8.803676642673592e+21\n",
      "5 1.5694215408839125e+22\n",
      "6 2.7977901426447888e+22\n",
      "7 4.987589043713365e+22\n",
      "8 8.891318934112022e+22\n",
      "9 1.585045433679926e+23\n",
      "10 2.8256426807396873e+23\n",
      "11 5.037241450348267e+23\n",
      "12 8.979833721390575e+23\n",
      "13 1.6008248653287842e+24\n",
      "14 2.8537724961994965e+24\n",
      "15 5.087388156225356e+24\n",
      "16 9.06922969037289e+24\n",
      "17 1.616761384249637e+25\n",
      "18 2.88218234937363e+25\n",
      "19 5.1380340821885045e+25\n",
      "20 9.159515613392016e+25\n",
      "21 1.632856554276437e+26\n",
      "22 2.9108750280913003e+26\n",
      "23 5.1891841980695174e+26\n",
      "24 9.250700350111284e+26\n",
      "25 1.6491119548114102e+27\n",
      "26 2.939853347935105e+27\n",
      "27 5.240843523175851e+27\n",
      "28 9.342792848393662e+27\n",
      "29 1.665529180980031e+28\n",
      "30 2.9691201525173e+28\n",
      "31 5.293017126783114e+28\n",
      "32 9.435802145179816e+28\n",
      "33 1.682109843787557e+29\n",
      "34 2.9986783137588523e+29\n",
      "35 5.3457101286325364e+29\n",
      "36 9.529737367374902e+29\n",
      "37 1.6988555702771104e+30\n",
      "38 3.028530732171252e+30\n",
      "39 5.398927699433356e+30\n",
      "40 9.624607732744152e+30\n",
      "41 1.7157680036893402e+31\n",
      "42 3.0586803371411337e+31\n",
      "43 5.452675061370201e+31\n",
      "44 9.720422550817426e+31\n",
      "45 1.732848803623674e+32\n",
      "46 3.0891300872177475e+32\n",
      "47 5.506957488615572e+32\n",
      "48 9.817191223802764e+32\n",
      "49 1.750099646201169e+33\n",
      "50 3.1198829704032593e+33\n",
      "51 5.561780307847346e+33\n",
      "52 9.914923247508942e+33\n",
      "53 1.7675222242289878e+34\n",
      "54 3.1509420044459805e+34\n",
      "55 5.617148898771521e+34\n",
      "56 1.0013628212277386e+35\n",
      "57 1.785118247366514e+35\n",
      "58 3.1823102371364752e+35\n",
      "59 5.67306869465009e+35\n",
      "60 1.0113315803923192e+36\n",
      "61 1.8028894422931212e+36\n",
      "62 3.2139907466066566e+36\n",
      "63 5.729545182834212e+36\n",
      "64 1.02139958046856e+37\n",
      "65 1.8208375528776028e+37\n",
      "66 3.245986641631825e+37\n",
      "67 5.786583905302682e+37\n",
      "68 1.031567809418792e+38\n",
      "69 1.838964340349305e+38\n",
      "70 3.2783010619357423e+38\n",
      "71 5.844190459205759e+38\n",
      "72 1.0418372650407012e+39\n",
      "73 1.8572715834709472e+39\n",
      "74 3.3109371784987196e+39\n",
      "75 5.902370497414407e+39\n",
      "76 1.0522089550652424e+40\n",
      "77 1.8757610787131795e+40\n",
      "78 3.3438981938687907e+40\n",
      "79 5.961129729075019e+40\n"
     ]
    }
   ],
   "source": [
    "for k in range(80):\n",
    "        \n",
    "    #forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum(((yout - ygt )**2 for ygt, yout in zip(ys, ypred)))\n",
    "\n",
    "    #backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "        \n",
    "    loss.backpropagate()\n",
    "    \n",
    "    #update\n",
    "    for p in n.parameters():\n",
    "        #print(p.grad)\n",
    "        p.value -= (0.05 * p.grad)\n",
    "        \n",
    "    print(k, loss.value)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea3867-9969-4188-a6fc-268d279827c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca55d5-74c6-442a-b1c4-2b432a3cba07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
